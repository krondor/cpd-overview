{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Classify tumors with machine learning\n",
    " \n",
    "This notebook explains how to use machine learning to classify tumor data. You develop your solution in three parts as follows:\n",
    "\n",
    "* An intuitive introduction to supervised learning concepts\n",
    "\n",
    "* A basic example of a machine learning model. \n",
    "\n",
    "* A deep dive into model stacking and parameter tuning, both of which are used in practice to significantly improve predictive accuracy\n",
    "\n",
    "Some guidelines for reading this notebook:\n",
    "\n",
    "* If you have no experience with machine learning: follow the entire notebook for a comprehensive walkthrough.\n",
    "\n",
    "* If you would like to read about decision trees, go to [section 2](#bullet-8).\n",
    "\n",
    "* If you would like to read about using XGBoost in practice, go to [section 3](#bullet-13).\n",
    "\n",
    "Some familiarity with Python is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Table of contents\n",
    "1.0 [Introduction to supervised learning](#bullet-1)<br/>\n",
    "2.0 [Basic model: decision trees](#bullet-8)<br/> \n",
    "3.0 [Ensemble model: gradient boosting](#bullet-13)  \n",
    "4.0 [XGBoost: parameter tuning](#bullet-17)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.0 Introduction to supervised learning<a class=\"anchor\" id=\"bullet-1\"></a>\n",
    "\n",
    "1.1 [What is machine learning?](#bullet-2)<br/>\n",
    "    1.2 [Defining the task](#bullet-3)<br/>\n",
    "    1.3 [How does an algorithm learn?](#bullet-4)<br/>\n",
    "    1.4 [Data preview](#bullet-5)<br/>\n",
    "    1.5 [Pre-processing](#bullet-6)<br/>\n",
    "    1.7 [Create train and test sets](#bullet-7)<br/>\n",
    "\n",
    "### 1.1 What is machine learning?<a class=\"anchor\" id=\"bullet-2\"></a>\n",
    "\n",
    "You use an algorithm (code) to create a model (mathematical function). The algorithm tries to find patterns in a sample of **training data** that can be used on future data. The model itself is a summary of these patterns, distilled into mathematical relationships between variables. The algorithm depends on **hyperparameters** that control how it looks for these patterns (that is, how it learns). \n",
    "\n",
    "The difficult part is to find parameters that balance accuracy and precision of the model with its ability to generalize.\n",
    "\n",
    "**Note:**\n",
    "* Accuracy and precision of predictions depend on the complexity of the model\n",
    "* Ability to generalize depends on how conservative the model is (how quickly it concludes that something is a pattern)\n",
    "\n",
    "This balancing act is called the **bias-variance tradeoff**, because a very complex model will represent all the specific nuances of its training data, therefore failing to generalize. In the opposite case, a simple model does not pick up enough of a pattern to ensure accuracy and precision of future predictions. \n",
    "\n",
    "Your job* is to pick an efficient algorithm and suitable model for our data set and learning task, and fine-tune its parameters to produce a balanced model.\n",
    "\n",
    "**apart from the arduous task of data pre-processing, which this notebook skims over for the sake of brevity (and sanity!)*\n",
    "\n",
    "\n",
    "### 1.2 Defining the task<a class=\"anchor\" id=\"bullet-3\"></a>\n",
    "\n",
    "The goal is to use an augmented version <a href=\"https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\" target=\"_blank\" rel=\"noopener noreferrer\">this data set</a> to develop a predictive model which classifies breast tumors as malignant or benign depending on measurements of the tumor cells. The [augmented data set](https://apsportal.ibm.com/exchange/public/entry/view/c173693bf48aeb22e41bbe2b41d79c1f) includes statistical analysis values, such as the mean, for some of the measurements. In machine learning terms, the **target** (what you want to predict) is the diagnosis, and the **features** are the measurements. The data sample contains **observations** of tumor cases described by these two components.\n",
    "\n",
    "\n",
    "### 1.3 How does an algorithm learn?<a class=\"anchor\" id=\"bullet-4\"></a>\n",
    "\n",
    "1. Initialize the algorithm to produce a default model, and give it the feature data. The algorithm makes its first set of predictions (close to a random guess).\n",
    "2. The algorithm measures the error between its previous prediction and the true value of the targets. \n",
    "3. The algorithm adjusts its model-building to make the error smaller. Each adjustment represents a part of the pattern it is learning, and is used to process future data.\n",
    "4. It continues to predict, calculate error, and adjust.\n",
    "\n",
    "Remember that the algorithm is only using a training sample to perform step 3. If the algorithm makes an adjustment so that its predictions are *exactly* the targets, then it is **overfitted** to the training data. This means that it has lost the ability to generalize to new (unseen) observations, because it picked up a pattern that only applies to the data points it learned from.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Load data\n",
    "To load the data:\n",
    "1. Go to the [data set](https://apsportal.ibm.com/exchange/public/entry/view/c173693bf48aeb22e41bbe2b41d79c1f) on the Watson Studio Community. \n",
    "1. Click the download icon and save the data set to your computer.  \n",
    "1. Load the `BreastCancerWisconsonDataSet.csv` file into your notebook. Click the **Data** icon on the notebook action bar. Drop the file into the box or browse to select the file. The file is loaded to your object storage and appears in the Data Assets section of the project. For more information, see <a href=\"https://datascience.ibm.com/docs/content/analyze-data/load-and-access-data.html\" target=\"_blank\" rel=\"noopener noreferrer\">Load and access data</a>.\n",
    "1. To load the data from the `BreastCancerWisconsonDataSet.csv` file into a pandas DataFrame, click in the next code cell and select **Insert to code > pandas DataFrame** under the file name.\n",
    "1. Make sure the object is named `df` instead of `df_data_1` in the last two lines of the **Insert to Code** block and then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8670</td>\n",
       "      <td>M</td>\n",
       "      <td>15.46</td>\n",
       "      <td>19.48</td>\n",
       "      <td>101.70</td>\n",
       "      <td>748.9</td>\n",
       "      <td>0.10920</td>\n",
       "      <td>0.12230</td>\n",
       "      <td>0.14660</td>\n",
       "      <td>0.08087</td>\n",
       "      <td>...</td>\n",
       "      <td>19.26</td>\n",
       "      <td>26.00</td>\n",
       "      <td>124.90</td>\n",
       "      <td>1156.0</td>\n",
       "      <td>0.15460</td>\n",
       "      <td>0.2394</td>\n",
       "      <td>0.3791</td>\n",
       "      <td>0.15140</td>\n",
       "      <td>0.2837</td>\n",
       "      <td>0.08019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8913</td>\n",
       "      <td>B</td>\n",
       "      <td>12.89</td>\n",
       "      <td>13.12</td>\n",
       "      <td>81.89</td>\n",
       "      <td>515.9</td>\n",
       "      <td>0.06955</td>\n",
       "      <td>0.03729</td>\n",
       "      <td>0.02260</td>\n",
       "      <td>0.01171</td>\n",
       "      <td>...</td>\n",
       "      <td>13.62</td>\n",
       "      <td>15.54</td>\n",
       "      <td>87.40</td>\n",
       "      <td>577.0</td>\n",
       "      <td>0.09616</td>\n",
       "      <td>0.1147</td>\n",
       "      <td>0.1186</td>\n",
       "      <td>0.05366</td>\n",
       "      <td>0.2309</td>\n",
       "      <td>0.06915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8915</td>\n",
       "      <td>B</td>\n",
       "      <td>14.96</td>\n",
       "      <td>19.10</td>\n",
       "      <td>97.03</td>\n",
       "      <td>687.3</td>\n",
       "      <td>0.08992</td>\n",
       "      <td>0.09823</td>\n",
       "      <td>0.05940</td>\n",
       "      <td>0.04819</td>\n",
       "      <td>...</td>\n",
       "      <td>16.25</td>\n",
       "      <td>26.19</td>\n",
       "      <td>109.10</td>\n",
       "      <td>809.8</td>\n",
       "      <td>0.13130</td>\n",
       "      <td>0.3030</td>\n",
       "      <td>0.1804</td>\n",
       "      <td>0.14890</td>\n",
       "      <td>0.2962</td>\n",
       "      <td>0.08472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9047</td>\n",
       "      <td>B</td>\n",
       "      <td>12.94</td>\n",
       "      <td>16.17</td>\n",
       "      <td>83.18</td>\n",
       "      <td>507.6</td>\n",
       "      <td>0.09879</td>\n",
       "      <td>0.08836</td>\n",
       "      <td>0.03296</td>\n",
       "      <td>0.02390</td>\n",
       "      <td>...</td>\n",
       "      <td>13.86</td>\n",
       "      <td>23.02</td>\n",
       "      <td>89.69</td>\n",
       "      <td>580.9</td>\n",
       "      <td>0.11720</td>\n",
       "      <td>0.1958</td>\n",
       "      <td>0.1810</td>\n",
       "      <td>0.08388</td>\n",
       "      <td>0.3297</td>\n",
       "      <td>0.07834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85715</td>\n",
       "      <td>M</td>\n",
       "      <td>13.17</td>\n",
       "      <td>18.66</td>\n",
       "      <td>85.98</td>\n",
       "      <td>534.6</td>\n",
       "      <td>0.11580</td>\n",
       "      <td>0.12310</td>\n",
       "      <td>0.12260</td>\n",
       "      <td>0.07340</td>\n",
       "      <td>...</td>\n",
       "      <td>15.67</td>\n",
       "      <td>27.95</td>\n",
       "      <td>102.80</td>\n",
       "      <td>759.4</td>\n",
       "      <td>0.17860</td>\n",
       "      <td>0.4166</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.20880</td>\n",
       "      <td>0.3900</td>\n",
       "      <td>0.11790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0   8670         M        15.46         19.48          101.70      748.9   \n",
       "1   8913         B        12.89         13.12           81.89      515.9   \n",
       "2   8915         B        14.96         19.10           97.03      687.3   \n",
       "3   9047         B        12.94         16.17           83.18      507.6   \n",
       "4  85715         M        13.17         18.66           85.98      534.6   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.10920           0.12230         0.14660              0.08087   \n",
       "1          0.06955           0.03729         0.02260              0.01171   \n",
       "2          0.08992           0.09823         0.05940              0.04819   \n",
       "3          0.09879           0.08836         0.03296              0.02390   \n",
       "4          0.11580           0.12310         0.12260              0.07340   \n",
       "\n",
       "            ...             radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           ...                    19.26          26.00           124.90   \n",
       "1           ...                    13.62          15.54            87.40   \n",
       "2           ...                    16.25          26.19           109.10   \n",
       "3           ...                    13.86          23.02            89.69   \n",
       "4           ...                    15.67          27.95           102.80   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      1156.0           0.15460             0.2394           0.3791   \n",
       "1       577.0           0.09616             0.1147           0.1186   \n",
       "2       809.8           0.13130             0.3030           0.1804   \n",
       "3       580.9           0.11720             0.1958           0.1810   \n",
       "4       759.4           0.17860             0.4166           0.5006   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0               0.15140          0.2837                  0.08019  \n",
       "1               0.05366          0.2309                  0.06915  \n",
       "2               0.14890          0.2962                  0.08472  \n",
       "3               0.08388          0.3297                  0.07834  \n",
       "4               0.20880          0.3900                  0.11790  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# @hidden_cell\n",
    "# This connection object is used to access your data and contains your credentials.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "\n",
    "from project_lib import Project\n",
    "project = Project.access()\n",
    "BreastCancerMalignancy_credentials = project.get_connected_data(name=\"BreastCancerMalignancy\")\n",
    "\n",
    "import os, jaydebeapi, pandas as pd\n",
    "\n",
    "BreastCancerMalignancy_port = BreastCancerMalignancy_credentials.get('port', 50000)\n",
    "\n",
    "BreastCancerMalignancy_connect_string = '{}://{}:{}/{}'.format(\n",
    "    'jdbc:db2',\n",
    "    BreastCancerMalignancy_credentials['host'],\n",
    "    BreastCancerMalignancy_port,\n",
    "    BreastCancerMalignancy_credentials['database']\n",
    ")\n",
    "\n",
    "if(BreastCancerMalignancy_credentials.get('ssl','false') == 'true'):\n",
    "    BreastCancerMalignancy_connect_string = BreastCancerMalignancy_connect_string + ':sslConnection=true;'\n",
    "    if BreastCancerMalignancy_credentials.get('ssl_certificate', None):\n",
    "        cert_file_path = os.path.join(os.path.expanduser('~'),'BreastCancerMalignancy_ssl.cert')\n",
    "        f = open(cert_file_path, \"w\")\n",
    "        f.write(BreastCancerMalignancy_credentials['ssl_certificate'])\n",
    "        f.close()\n",
    "        BreastCancerMalignancy_connect_string = BreastCancerMalignancy_connect_string + 'sslCertLocation=' + cert_file_path + ';'\n",
    "\n",
    "BreastCancerMalignancy_connection = jaydebeapi.connect(\n",
    "    'com.ibm.db2.jcc.DB2Driver',\n",
    "    BreastCancerMalignancy_connect_string,\n",
    "    [BreastCancerMalignancy_credentials['username'],BreastCancerMalignancy_credentials['password']]\n",
    ")\n",
    "\n",
    "# In the following query, the schema and table name are in quotes in order to support case-sensitive names or labels\n",
    "query = 'SELECT * FROM \"EDW\".\"BREASTCANCER\"'\n",
    "# If your database does not support case-sensitive names or quoted names, you can use the following query:\n",
    "#query = 'SELECT * FROM EDW.BREASTCANCER'\n",
    "\n",
    "df = pd.read_sql(query, con=BreastCancerMalignancy_connection)\n",
    "df.head()\n",
    "\n",
    "# Close the database connection with the following code:\n",
    "# BreastCancerMalignancy_connection.close()\n",
    "# To learn more about the jaydebeapi package, please read the documentation: https://pypi.org/project/JayDeBeApi/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.4 Data preview<a class=\"anchor\" id=\"bullet-5\"></a>\n",
    "\n",
    "In the following data table, each row corresponds to one observation of a tumor. The columns contain the measurement features, with one column containing the diagnosis target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>887549</td>\n",
       "      <td>M</td>\n",
       "      <td>20.31</td>\n",
       "      <td>27.06</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1288.0</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.10880</td>\n",
       "      <td>0.15190</td>\n",
       "      <td>0.09333</td>\n",
       "      <td>...</td>\n",
       "      <td>24.33</td>\n",
       "      <td>39.16</td>\n",
       "      <td>162.30</td>\n",
       "      <td>1844.0</td>\n",
       "      <td>0.15220</td>\n",
       "      <td>0.29450</td>\n",
       "      <td>0.37880</td>\n",
       "      <td>0.16970</td>\n",
       "      <td>0.3151</td>\n",
       "      <td>0.07999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>888264</td>\n",
       "      <td>M</td>\n",
       "      <td>17.35</td>\n",
       "      <td>23.06</td>\n",
       "      <td>111.00</td>\n",
       "      <td>933.1</td>\n",
       "      <td>0.08662</td>\n",
       "      <td>0.06290</td>\n",
       "      <td>0.02891</td>\n",
       "      <td>0.02837</td>\n",
       "      <td>...</td>\n",
       "      <td>19.85</td>\n",
       "      <td>31.47</td>\n",
       "      <td>128.20</td>\n",
       "      <td>1218.0</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0.14860</td>\n",
       "      <td>0.12110</td>\n",
       "      <td>0.08235</td>\n",
       "      <td>0.2452</td>\n",
       "      <td>0.06515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>888570</td>\n",
       "      <td>M</td>\n",
       "      <td>17.29</td>\n",
       "      <td>22.13</td>\n",
       "      <td>114.40</td>\n",
       "      <td>947.8</td>\n",
       "      <td>0.08999</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.09697</td>\n",
       "      <td>0.07507</td>\n",
       "      <td>...</td>\n",
       "      <td>20.39</td>\n",
       "      <td>27.24</td>\n",
       "      <td>137.90</td>\n",
       "      <td>1295.0</td>\n",
       "      <td>0.11340</td>\n",
       "      <td>0.28670</td>\n",
       "      <td>0.22980</td>\n",
       "      <td>0.15280</td>\n",
       "      <td>0.3067</td>\n",
       "      <td>0.07484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>889403</td>\n",
       "      <td>M</td>\n",
       "      <td>15.61</td>\n",
       "      <td>19.38</td>\n",
       "      <td>100.00</td>\n",
       "      <td>758.6</td>\n",
       "      <td>0.07840</td>\n",
       "      <td>0.05616</td>\n",
       "      <td>0.04209</td>\n",
       "      <td>0.02847</td>\n",
       "      <td>...</td>\n",
       "      <td>17.91</td>\n",
       "      <td>31.67</td>\n",
       "      <td>115.90</td>\n",
       "      <td>988.6</td>\n",
       "      <td>0.10840</td>\n",
       "      <td>0.18070</td>\n",
       "      <td>0.22600</td>\n",
       "      <td>0.08568</td>\n",
       "      <td>0.2683</td>\n",
       "      <td>0.06829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>889719</td>\n",
       "      <td>M</td>\n",
       "      <td>17.19</td>\n",
       "      <td>22.07</td>\n",
       "      <td>111.60</td>\n",
       "      <td>928.3</td>\n",
       "      <td>0.09726</td>\n",
       "      <td>0.08995</td>\n",
       "      <td>0.09061</td>\n",
       "      <td>0.06527</td>\n",
       "      <td>...</td>\n",
       "      <td>21.58</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.50</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>0.15580</td>\n",
       "      <td>0.25670</td>\n",
       "      <td>0.38890</td>\n",
       "      <td>0.19840</td>\n",
       "      <td>0.3216</td>\n",
       "      <td>0.07570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>891670</td>\n",
       "      <td>B</td>\n",
       "      <td>12.95</td>\n",
       "      <td>16.02</td>\n",
       "      <td>83.14</td>\n",
       "      <td>513.7</td>\n",
       "      <td>0.10050</td>\n",
       "      <td>0.07943</td>\n",
       "      <td>0.06155</td>\n",
       "      <td>0.03370</td>\n",
       "      <td>...</td>\n",
       "      <td>13.74</td>\n",
       "      <td>19.93</td>\n",
       "      <td>88.81</td>\n",
       "      <td>585.4</td>\n",
       "      <td>0.14830</td>\n",
       "      <td>0.20680</td>\n",
       "      <td>0.22410</td>\n",
       "      <td>0.10560</td>\n",
       "      <td>0.3380</td>\n",
       "      <td>0.09584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>891703</td>\n",
       "      <td>B</td>\n",
       "      <td>11.85</td>\n",
       "      <td>17.46</td>\n",
       "      <td>75.54</td>\n",
       "      <td>432.7</td>\n",
       "      <td>0.08372</td>\n",
       "      <td>0.05642</td>\n",
       "      <td>0.02688</td>\n",
       "      <td>0.02280</td>\n",
       "      <td>...</td>\n",
       "      <td>13.06</td>\n",
       "      <td>25.75</td>\n",
       "      <td>84.35</td>\n",
       "      <td>517.8</td>\n",
       "      <td>0.13690</td>\n",
       "      <td>0.17580</td>\n",
       "      <td>0.13160</td>\n",
       "      <td>0.09140</td>\n",
       "      <td>0.3101</td>\n",
       "      <td>0.07007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>891716</td>\n",
       "      <td>B</td>\n",
       "      <td>12.72</td>\n",
       "      <td>13.78</td>\n",
       "      <td>81.78</td>\n",
       "      <td>492.1</td>\n",
       "      <td>0.09667</td>\n",
       "      <td>0.08393</td>\n",
       "      <td>0.01288</td>\n",
       "      <td>0.01924</td>\n",
       "      <td>...</td>\n",
       "      <td>13.50</td>\n",
       "      <td>17.48</td>\n",
       "      <td>88.54</td>\n",
       "      <td>553.7</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.14720</td>\n",
       "      <td>0.05233</td>\n",
       "      <td>0.06343</td>\n",
       "      <td>0.2369</td>\n",
       "      <td>0.06922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>891923</td>\n",
       "      <td>B</td>\n",
       "      <td>13.77</td>\n",
       "      <td>13.27</td>\n",
       "      <td>88.06</td>\n",
       "      <td>582.7</td>\n",
       "      <td>0.09198</td>\n",
       "      <td>0.06221</td>\n",
       "      <td>0.01063</td>\n",
       "      <td>0.01917</td>\n",
       "      <td>...</td>\n",
       "      <td>14.67</td>\n",
       "      <td>16.93</td>\n",
       "      <td>94.17</td>\n",
       "      <td>661.1</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0.03732</td>\n",
       "      <td>0.05802</td>\n",
       "      <td>0.2823</td>\n",
       "      <td>0.06794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>891936</td>\n",
       "      <td>B</td>\n",
       "      <td>10.91</td>\n",
       "      <td>12.35</td>\n",
       "      <td>69.14</td>\n",
       "      <td>363.7</td>\n",
       "      <td>0.08518</td>\n",
       "      <td>0.04721</td>\n",
       "      <td>0.01236</td>\n",
       "      <td>0.01369</td>\n",
       "      <td>...</td>\n",
       "      <td>11.37</td>\n",
       "      <td>14.82</td>\n",
       "      <td>72.42</td>\n",
       "      <td>392.2</td>\n",
       "      <td>0.09312</td>\n",
       "      <td>0.07506</td>\n",
       "      <td>0.02884</td>\n",
       "      <td>0.03194</td>\n",
       "      <td>0.2143</td>\n",
       "      <td>0.06643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "200  887549         M        20.31         27.06          132.90     1288.0   \n",
       "201  888264         M        17.35         23.06          111.00      933.1   \n",
       "202  888570         M        17.29         22.13          114.40      947.8   \n",
       "203  889403         M        15.61         19.38          100.00      758.6   \n",
       "204  889719         M        17.19         22.07          111.60      928.3   \n",
       "205  891670         B        12.95         16.02           83.14      513.7   \n",
       "206  891703         B        11.85         17.46           75.54      432.7   \n",
       "207  891716         B        12.72         13.78           81.78      492.1   \n",
       "208  891923         B        13.77         13.27           88.06      582.7   \n",
       "209  891936         B        10.91         12.35           69.14      363.7   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "200          0.10000           0.10880         0.15190              0.09333   \n",
       "201          0.08662           0.06290         0.02891              0.02837   \n",
       "202          0.08999           0.12730         0.09697              0.07507   \n",
       "203          0.07840           0.05616         0.04209              0.02847   \n",
       "204          0.09726           0.08995         0.09061              0.06527   \n",
       "205          0.10050           0.07943         0.06155              0.03370   \n",
       "206          0.08372           0.05642         0.02688              0.02280   \n",
       "207          0.09667           0.08393         0.01288              0.01924   \n",
       "208          0.09198           0.06221         0.01063              0.01917   \n",
       "209          0.08518           0.04721         0.01236              0.01369   \n",
       "\n",
       "              ...             radius_worst  texture_worst  perimeter_worst  \\\n",
       "200           ...                    24.33          39.16           162.30   \n",
       "201           ...                    19.85          31.47           128.20   \n",
       "202           ...                    20.39          27.24           137.90   \n",
       "203           ...                    17.91          31.67           115.90   \n",
       "204           ...                    21.58          29.33           140.50   \n",
       "205           ...                    13.74          19.93            88.81   \n",
       "206           ...                    13.06          25.75            84.35   \n",
       "207           ...                    13.50          17.48            88.54   \n",
       "208           ...                    14.67          16.93            94.17   \n",
       "209           ...                    11.37          14.82            72.42   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "200      1844.0           0.15220            0.29450          0.37880   \n",
       "201      1218.0           0.12400            0.14860          0.12110   \n",
       "202      1295.0           0.11340            0.28670          0.22980   \n",
       "203       988.6           0.10840            0.18070          0.22600   \n",
       "204      1436.0           0.15580            0.25670          0.38890   \n",
       "205       585.4           0.14830            0.20680          0.22410   \n",
       "206       517.8           0.13690            0.17580          0.13160   \n",
       "207       553.7           0.12980            0.14720          0.05233   \n",
       "208       661.1           0.11700            0.10720          0.03732   \n",
       "209       392.2           0.09312            0.07506          0.02884   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "200               0.16970          0.3151                  0.07999  \n",
       "201               0.08235          0.2452                  0.06515  \n",
       "202               0.15280          0.3067                  0.07484  \n",
       "203               0.08568          0.2683                  0.06829  \n",
       "204               0.19840          0.3216                  0.07570  \n",
       "205               0.10560          0.3380                  0.09584  \n",
       "206               0.09140          0.3101                  0.07007  \n",
       "207               0.06343          0.2369                  0.06922  \n",
       "208               0.05802          0.2823                  0.06794  \n",
       "209               0.03194          0.2143                  0.06643  \n",
       "\n",
       "[10 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "print (df.shape)\n",
    "display(df[200:210])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The data contains features extracted from 569 diagnostic images of breast tumors. The diagnosis column indicates whether the mass was benign (B), or malignant (M). The rest of the columns contain features which are structured as follows:\n",
    "\n",
    "10 variables describe the cell nuclei of each mass, and for each variable, the mean, standard deviation, and 'worst' (mean of three largest measurements) are calculated. The variables are briefly described on the <a href=\"https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\" target=\"_blank\" rel=\"noopener noreferrer\">original data set page</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.5 Pre-processing<a class=\"anchor\" id=\"bullet-6\"></a>\n",
    "\n",
    "The following code cell gently cleans the data and generates a summary that can be used to check for outliers. This data set is well prepared for analysis and does not require further manipulation.\n",
    "\n",
    "If you don't have scikit-learn library installed, install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached https://files.pythonhosted.org/packages/dc/8f/416ccf81408cd8ea84be2a38efe34cc885966c4b6edbe705d2642e22d208/scikit_learn-0.20.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /opt/conda3/lib/python3.5/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /opt/conda3/lib/python3.5/site-packages (from scikit-learn) (1.14.5)\n",
      "Installing collected packages: scikit-learn\n",
      "  Found existing installation: scikit-learn 0.19.2\n",
      "    Uninstalling scikit-learn-0.19.2:\n",
      "      Successfully uninstalled scikit-learn-0.19.2\n",
      "Successfully installed scikit-learn-0.20.0\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.037183e+07</td>\n",
       "      <td>0.372583</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>...</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.250206e+08</td>\n",
       "      <td>0.483918</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>...</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.670000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.692180e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>...</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.060240e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.813129e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>...</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.113205e+08</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>...</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id   diagnosis  radius_mean  texture_mean  perimeter_mean  \\\n",
       "count  5.690000e+02  569.000000   569.000000    569.000000      569.000000   \n",
       "mean   3.037183e+07    0.372583    14.127292     19.289649       91.969033   \n",
       "std    1.250206e+08    0.483918     3.524049      4.301036       24.298981   \n",
       "min    8.670000e+03    0.000000     6.981000      9.710000       43.790000   \n",
       "25%    8.692180e+05    0.000000    11.700000     16.170000       75.170000   \n",
       "50%    9.060240e+05    0.000000    13.370000     18.840000       86.240000   \n",
       "75%    8.813129e+06    1.000000    15.780000     21.800000      104.100000   \n",
       "max    9.113205e+08    1.000000    28.110000     39.280000      188.500000   \n",
       "\n",
       "         area_mean  smoothness_mean  compactness_mean  concavity_mean  \\\n",
       "count   569.000000       569.000000        569.000000      569.000000   \n",
       "mean    654.889104         0.096360          0.104341        0.088799   \n",
       "std     351.914129         0.014064          0.052813        0.079720   \n",
       "min     143.500000         0.052630          0.019380        0.000000   \n",
       "25%     420.300000         0.086370          0.064920        0.029560   \n",
       "50%     551.100000         0.095870          0.092630        0.061540   \n",
       "75%     782.700000         0.105300          0.130400        0.130700   \n",
       "max    2501.000000         0.163400          0.345400        0.426800   \n",
       "\n",
       "       concave points_mean           ...             radius_worst  \\\n",
       "count           569.000000           ...               569.000000   \n",
       "mean              0.048919           ...                16.269190   \n",
       "std               0.038803           ...                 4.833242   \n",
       "min               0.000000           ...                 7.930000   \n",
       "25%               0.020310           ...                13.010000   \n",
       "50%               0.033500           ...                14.970000   \n",
       "75%               0.074000           ...                18.790000   \n",
       "max               0.201200           ...                36.040000   \n",
       "\n",
       "       texture_worst  perimeter_worst   area_worst  smoothness_worst  \\\n",
       "count     569.000000       569.000000   569.000000        569.000000   \n",
       "mean       25.677223       107.261213   880.583128          0.132369   \n",
       "std         6.146258        33.602542   569.356993          0.022832   \n",
       "min        12.020000        50.410000   185.200000          0.071170   \n",
       "25%        21.080000        84.110000   515.300000          0.116600   \n",
       "50%        25.410000        97.660000   686.500000          0.131300   \n",
       "75%        29.720000       125.400000  1084.000000          0.146000   \n",
       "max        49.540000       251.200000  4254.000000          0.222600   \n",
       "\n",
       "       compactness_worst  concavity_worst  concave points_worst  \\\n",
       "count         569.000000       569.000000            569.000000   \n",
       "mean            0.254265         0.272188              0.114606   \n",
       "std             0.157336         0.208624              0.065732   \n",
       "min             0.027290         0.000000              0.000000   \n",
       "25%             0.147200         0.114500              0.064930   \n",
       "50%             0.211900         0.226700              0.099930   \n",
       "75%             0.339100         0.382900              0.161400   \n",
       "max             1.058000         1.252000              0.291000   \n",
       "\n",
       "       symmetry_worst  fractal_dimension_worst  \n",
       "count      569.000000               569.000000  \n",
       "mean         0.290076                 0.083946  \n",
       "std          0.061867                 0.018061  \n",
       "min          0.156500                 0.055040  \n",
       "25%          0.250400                 0.071460  \n",
       "50%          0.282200                 0.080040  \n",
       "75%          0.317900                 0.092080  \n",
       "max          0.663800                 0.207500  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# checking for missing values\n",
    "df.isnull().any()\n",
    "\n",
    "# dropping Unnamed column\n",
    "#df2 = df.drop(['Unnamed: 32'], axis=1)\n",
    "\n",
    "# converting diagnosis M/B to numerical\n",
    "lenc = LabelEncoder()\n",
    "lenc.fit(df['diagnosis'])\n",
    "df['diagnosis'] = lenc.transform(df['diagnosis'])\n",
    "\n",
    "# overview of data sample\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.6 Create train and test sets<a class=\"anchor\" id=\"bullet-7\"></a>\n",
    "\n",
    "Take 80% of the total data to train the model, and reserve 20% of it for testing. After the model is built and tuned upon the training set, you can use the testing set to observe how well the model is able to generalize to new 'unseen' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:5: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## X are all the features (columns) that might be useful to the model\n",
    "## y is the target (diagnosis column)\n",
    "X, y = df.ix[:,'radius_mean':], df[['diagnosis']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.0 Basic model: decision trees<a class=\"anchor\" id=\"bullet-8\"></a>\n",
    "\n",
    "2.1 [Mimicking human decision-making](#bullet-9)<br/>\n",
    "    2.2 [Interpreting a tree graphic](#bullet-10)<br/>\n",
    "    2.3 [How a decision tree learns](#bullet-11)<br/>\n",
    "    2.3 [Evaluating the model](#bullet-12)<br/>\n",
    "\n",
    "### 2.1 Mimicking human decision-making<a class=\"anchor\" id=\"bullet-9\"></a>\n",
    "\n",
    "A decision tree is an algorithm that can be used for machine learning. It is analogous to the game <a href=\"http://www.wikihow.com/Play-20-Questions\" target=\"_blank\" rel=\"noopener noreferrer\">20 questions</a> - each 'question' is a **splitting feature** chosen from the data based on how useful it is for identifying the target. The algorithm decides on these features by optimizing a variety of mathematical functions that quantify prediction error (<a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning#Metrics\" target=\"_blank\" rel=\"noopener noreferrer\">for the curious</a>).\n",
    "\n",
    "Trees are considered weak learners, because their predictions are usually only slightly better than chance. The following code builds a tree based on the tumor data, and displays the process graphically.\n",
    "\n",
    "If you don't have pydotplus library installed, install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata .........\n",
      "Solving package specifications: ..........\n",
      "Warning: 2 possible package resolutions (only showing differing packages):\n",
      "  - conda-forge::icu-58.2-0.tar.bz2\n",
      "  - conda-forge::icu-58.2-hfc679d8_0.tar.bz2\n",
      "\n",
      "Package plan for installation in environment /opt/conda3:\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    icu-58.2                   |                0        22.7 MB  conda-forge\n",
      "    libstdcxx-ng-7.2.0         |       hdf63c60_3         2.5 MB  conda-forge\n",
      "    expat-2.2.5                |       hfc679d8_2         144 KB  conda-forge\n",
      "    gettext-0.19.8.1           |       h5e8e0c9_1         3.5 MB  conda-forge\n",
      "    graphite2-1.3.12           |       hfc679d8_1         104 KB  conda-forge\n",
      "    jpeg-9c                    |       h470a237_1         229 KB  conda-forge\n",
      "    libiconv-1.15              |       h470a237_3         2.0 MB  conda-forge\n",
      "    libpng-1.6.35              |       ha92aebf_2         305 KB  conda-forge\n",
      "    libtool-2.4.6              |       h470a237_2         517 KB  conda-forge\n",
      "    libuuid-2.32.1             |       h470a237_2          24 KB  conda-forge\n",
      "    pcre-8.41                  |       hfc679d8_3         243 KB  conda-forge\n",
      "    pixman-0.34.0              |       h470a237_3         1.2 MB  conda-forge\n",
      "    xorg-kbproto-1.0.7         |       h470a237_2          25 KB  conda-forge\n",
      "    xorg-libice-1.0.9          |       h470a237_4          55 KB  conda-forge\n",
      "    xorg-renderproto-0.11.1    |       h470a237_2           8 KB  conda-forge\n",
      "    xorg-xextproto-7.3.0       |       h470a237_2          26 KB  conda-forge\n",
      "    xorg-xproto-7.0.31         |       h470a237_7          72 KB  conda-forge\n",
      "    freetype-2.9.1             |       h6debe1e_4         800 KB  conda-forge\n",
      "    glib-2.56.2                |       h464dc38_0         4.9 MB  conda-forge\n",
      "    libtiff-4.0.9              |       he6b73bb_1         521 KB  conda-forge\n",
      "    libxml2-2.9.8              |       h422b904_2         1.8 MB  conda-forge\n",
      "    xorg-libsm-1.2.2           |       h8c8a85c_6          24 KB  conda-forge\n",
      "    xorg-libx11-1.6.6          |       h470a237_0         890 KB  conda-forge\n",
      "    fontconfig-2.13.1          |       h65d0f4c_0         320 KB  conda-forge\n",
      "    xorg-libxext-1.3.3         |       h470a237_4          45 KB  conda-forge\n",
      "    xorg-libxrender-0.9.10     |       h470a237_2          28 KB  conda-forge\n",
      "    xorg-libxt-1.1.5           |       h470a237_2         352 KB  conda-forge\n",
      "    cairo-1.14.12              |       h276e583_5         1.3 MB  conda-forge\n",
      "    xorg-libxpm-3.5.12         |       h470a237_2          56 KB  conda-forge\n",
      "    harfbuzz-1.9.0             |       hee26f79_1         1.1 MB  conda-forge\n",
      "    pango-1.40.14              |       he752989_2         527 KB  conda-forge\n",
      "    graphviz-2.38.0            |       h08bfae6_9         9.7 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        56.0 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    gettext:          0.19.8.1-h5e8e0c9_1 conda-forge\n",
      "    graphite2:        1.3.12-hfc679d8_1   conda-forge\n",
      "    graphviz:         2.38.0-h08bfae6_9   conda-forge\n",
      "    libiconv:         1.15-h470a237_3     conda-forge\n",
      "    libstdcxx-ng:     7.2.0-hdf63c60_3    conda-forge\n",
      "    libtool:          2.4.6-h470a237_2    conda-forge\n",
      "    libuuid:          2.32.1-h470a237_2   conda-forge\n",
      "    pango:            1.40.14-he752989_2  conda-forge\n",
      "    pcre:             8.41-hfc679d8_3     conda-forge\n",
      "    xorg-kbproto:     1.0.7-h470a237_2    conda-forge\n",
      "    xorg-libice:      1.0.9-h470a237_4    conda-forge\n",
      "    xorg-libsm:       1.2.2-h8c8a85c_6    conda-forge\n",
      "    xorg-libx11:      1.6.6-h470a237_0    conda-forge\n",
      "    xorg-libxext:     1.3.3-h470a237_4    conda-forge\n",
      "    xorg-libxpm:      3.5.12-h470a237_2   conda-forge\n",
      "    xorg-libxrender:  0.9.10-h470a237_2   conda-forge\n",
      "    xorg-libxt:       1.1.5-h470a237_2    conda-forge\n",
      "    xorg-renderproto: 0.11.1-h470a237_2   conda-forge\n",
      "    xorg-xextproto:   7.3.0-h470a237_2    conda-forge\n",
      "    xorg-xproto:      7.0.31-h470a237_7   conda-forge\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    cairo:            1.12.18-6                       --> 1.14.12-h276e583_5 conda-forge\n",
      "    expat:            2.1.0-0                         --> 2.2.5-hfc679d8_2   conda-forge\n",
      "    fontconfig:       2.11.1-6                        --> 2.13.1-h65d0f4c_0  conda-forge\n",
      "    freetype:         2.5.5-1                         --> 2.9.1-h6debe1e_4   conda-forge\n",
      "    glib:             2.43.0-1                        --> 2.56.2-h464dc38_0  conda-forge\n",
      "    harfbuzz:         0.9.39-1                        --> 1.9.0-hee26f79_1   conda-forge\n",
      "    icu:              54.1-0                          --> 58.2-0             conda-forge\n",
      "    jpeg:             8d-2                            --> 9c-h470a237_1      conda-forge\n",
      "    libpng:           1.6.22-0                        --> 1.6.35-ha92aebf_2  conda-forge\n",
      "    libtiff:          4.0.6-2                         --> 4.0.9-he6b73bb_1   conda-forge\n",
      "    libxml2:          2.9.2-0                         --> 2.9.8-h422b904_2   conda-forge\n",
      "    pixman:           0.32.6-0                        --> 0.34.0-h470a237_3  conda-forge\n",
      "\n",
      "Fetching packages ...\n",
      "icu-58.2-0.tar 100% |################################| Time: 0:00:00  29.60 MB/s\n",
      "libstdcxx-ng-7 100% |################################| Time: 0:00:00  10.57 MB/sime: 0:00:00   5.99 MB/sime: 0:00:00  10.05 MB/s\n",
      "expat-2.2.5-hf 100% |################################| Time: 0:00:00   1.48 MB/s\n",
      "gettext-0.19.8 100% |################################| Time: 0:00:00  12.72 MB/s\n",
      "graphite2-1.3. 100% |################################| Time: 0:00:00   1.15 MB/s\n",
      "jpeg-9c-h470a2 100% |################################| Time: 0:00:00   1.49 MB/s\n",
      "libiconv-1.15- 100% |################################| Time: 0:00:00   8.16 MB/sime: 0:00:00   6.59 MB/s\n",
      "libpng-1.6.35- 100% |################################| Time: 0:00:00   2.37 MB/s\n",
      "libtool-2.4.6- 100% |################################| Time: 0:00:00   3.34 MB/s\n",
      "libuuid-2.32.1 100% |################################| Time: 0:00:00 742.63 kB/s\n",
      "pcre-8.41-hfc6 100% |################################| Time: 0:00:00   1.98 MB/s\n",
      "pixman-0.34.0- 100% |################################| Time: 0:00:00   5.66 MB/s\n",
      "xorg-kbproto-1 100% |################################| Time: 0:00:00 834.37 kB/s\n",
      "xorg-libice-1. 100% |################################| Time: 0:00:00 930.48 kB/s\n",
      "xorg-renderpro 100% |################################| Time: 0:00:00   8.51 MB/s\n",
      "xorg-xextproto 100% |################################| Time: 0:00:00 918.72 kB/s\n",
      "xorg-xproto-7. 100% |################################| Time: 0:00:00   1.20 MB/s\n",
      "freetype-2.9.1 100% |################################| Time: 0:00:00   4.17 MB/s\n",
      "glib-2.56.2-h4 100% |################################| Time: 0:00:00  14.29 MB/sime: 0:00:00   5.18 MB/sime: 0:00:00   8.99 MB/s\n",
      "libtiff-4.0.9- 100% |################################| Time: 0:00:00   3.48 MB/s\n",
      "libxml2-2.9.8- 100% |################################| Time: 0:00:00   7.77 MB/s\n",
      "xorg-libsm-1.2 100% |################################| Time: 0:00:00 821.28 kB/s\n",
      "xorg-libx11-1. 100% |################################| Time: 0:00:00   4.94 MB/s\n",
      "fontconfig-2.1 100% |################################| Time: 0:00:00   2.68 MB/s\n",
      "xorg-libxext-1 100% |################################| Time: 0:00:00 722.28 kB/s\n",
      "xorg-libxrende 100% |################################| Time: 0:00:00 889.10 kB/s\n",
      "xorg-libxt-1.1 100% |################################| Time: 0:00:00   2.46 MB/s\n",
      "cairo-1.14.12- 100% |################################| Time: 0:00:00   6.96 MB/s\n",
      "xorg-libxpm-3. 100% |################################| Time: 0:00:00 885.64 kB/s\n",
      "harfbuzz-1.9.0 100% |################################| Time: 0:00:00   5.31 MB/s\n",
      "pango-1.40.14- 100% |################################| Time: 0:00:00   3.62 MB/s\n",
      "graphviz-2.38. 100% |################################| Time: 0:00:00  24.25 MB/sime: 0:00:00  10.73 MB/s####                  | Time: 0:00:00  15.91 MB/sime: 0:00:00  20.14 MB/sime: 0:00:00  23.30 MB/s\n",
      "Extracting packages ...\n",
      "[      COMPLETE      ]|###################################################| 100%\n",
      "Unlinking packages ...\n",
      "[      COMPLETE      ]|###################################################| 100%\n",
      "Linking packages ...\n",
      "CondaOSError: OS error: failed to link (src='/opt/conda3/pkgs/icu-58.2-0/lib/icu/current', dst='/opt/conda3/lib/icu/current', type=3, error=FileExistsError(17, 'File exists'))\n",
      "\n",
      "\n",
      "Requirement already satisfied: graphviz in /user-home/1002/.local/lib/python3.5/site-packages (0.9)\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pydotplus in /opt/conda3/lib/python3.5/site-packages (2.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in /opt/conda3/lib/python3.5/site-packages (from pydotplus) (2.1.4)\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!conda install -y graphviz\n",
    "!pip install graphviz\n",
    "!pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import pydotplus\n",
    "from IPython.display import Image \n",
    "\n",
    "# list of features we want to consider\n",
    "splitting_features = [x for x in X_train.columns if x not in ['id']]\n",
    "\n",
    "# initializing the tree model and training it\n",
    "tree_model = tree.DecisionTreeClassifier()\n",
    "tree_model = tree_model.fit(X_train, y_train)\n",
    "\n",
    "# generating a graphic for the tree\n",
    "dot_data = tree.export_graphviz(tree_model, out_file=None, \n",
    "                         feature_names=splitting_features,\n",
    "                         class_names = ['Benign', 'Malignant'],\n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2 Interpreting a tree graphic<a class=\"anchor\" id=\"bullet-10\"></a>\n",
    "Each rectangle represents a **node** of the tree.\n",
    "* The first line in each node identifies the splitting feature and decision being made upon it.\n",
    "* The observations are separated based on this decision into **children** nodes (left and right).\n",
    "* **Samples**: how many observations have been filtered by that decision.\n",
    "* **Value**: where how many observations are being split into the left and right children respectively.\n",
    "* **Gini**: a measurement of incorrect classification. As the algorithm learns, the gini inequality decreases to 0.\n",
    "* **Class**: the target that the observations have been grouped into: benign or malignant.\n",
    "* The rectangles at the bottom of the diagram are called **leaves**, which contain the final predictions (observations classified by diagnosis).\n",
    "\n",
    "### 2.3 How a decision tree learns<a class=\"anchor\" id=\"bullet-11\"></a>\n",
    "\n",
    "Applying the general learning outline ([section 1.3](#bullet-4)) to a decision tree:\n",
    "\n",
    "*1. Initialize the algorithm to produce a default model, and give it the feature data. The algorithm makes its first set of predictions (close to a random guess).*<br/>\n",
    "The default model is the very first splitting feature - the algorithm chose 'concave points mean <= 0.0492'. The observations are divided into benign or malignant based on this decision, making up the first set of predictions!\n",
    "\n",
    "*2. The algorithm measures the error between its previous prediction and the true value of the targets.*<br/> \n",
    "This error is quantified by the gini value that appears in the first node of the tree.\n",
    "\n",
    "*3. The algorithm adjusts its model-building to make the error smaller. Each adjustment represents a part of the pattern it is learning, and is used to process future data.*<br/>\n",
    "The adjustment is the next splitting feature that is chosen for each subgroup that resulted from the concave points mean decision. The algorithm chose 'radius worst' for observations that had a concave points mean less than the threshold, and 'concavity worst' for observations with concave points mean greater than the threshold.\n",
    "\n",
    "*4. It continues to predict, calculate error, and adjust.*<br/>\n",
    "The next set of predictions are the classifications that result from the second round of decision making. Each time, a gini value is calculated to indicate how far the classifications are from the true targets, and another set of splitting features are chosen to further refine the class groupings. \n",
    "\n",
    "### 2.4 Evaluating the model<a class=\"anchor\" id=\"bullet-12\"></a>\n",
    "Pick a metric to measure predictive accuracy or performance of your model, so that you can compare different models. \n",
    "\n",
    "Models can differ in two ways:\n",
    "\n",
    "1. The algorithm used for learning (for example, the difference between a decision tree and gradient boosting)\n",
    "2. The **parameter** selection within a single algorithm (for example, a model with a learning rate set to 0.1 or 0.0)\n",
    "\n",
    "This case refers to (1), you want to know how the tree model performs, so that it can be compared to the gradient boosting model in the next section.\n",
    "\n",
    "The choice of evaluation metric depends on the type of learning problem. For binary classification, you can use a metric called the <a href=\"https://www.quora.com/Machine-Learning-What-is-an-intuitive-explanation-of-AUC\" target=\"_blank\" rel=\"noopener noreferrer\">AUC score</a>, which measures the probability that the model correctly identifies *malignant tumors only*. This metric is used as an indication of performance because misclassifying a malignant tumor as benign is the worst prediction scenario (compared to correct classification of benign/malignant and misclassification of benign).\n",
    "\n",
    "You also calculate the general accuracy of the model - the percentage of observations that have been correctly classified overall. The following code uses your simple tree model to predict on the testing set, and evaluates the accuracy and AUC score of its predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9211\n",
      "AUC Score: 0.927579\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "#Predict test set:\n",
    "test_predictions = tree_model.predict(X_test[splitting_features])\n",
    "test_predprob = tree_model.predict_proba(X_test[splitting_features])[:,1]\n",
    "        \n",
    "#Print model report:\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_test['diagnosis'].values, test_predictions))\n",
    "print(\"AUC Score: %f\" % metrics.roc_auc_score(y_test['diagnosis'], test_predprob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The model gives ~92% accuracy for all cases, and a ~93% probability of catching malignant cases in the data. Each time the code is run, the algorithm may learn slightly differently (that is, select different splitting features), which results in slight variations in accuracy and AUC scores. In the following section, you work on improving the model by stacking together many trees, and tuning parameters for better generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3.0 Ensemble model: gradient tree boosting<a class=\"anchor\" id=\"bullet-13\"></a>\n",
    "\n",
    "Stacking, or meta-ensembling, is a method of joining multiple predictive models so that the strengths of each can cover the weaknesses of others. Gradient boosting is one technique used to stack weak learners together in an **ensemble** to achieve better predictions with each additional model.\n",
    "\n",
    "3.1 [An intuitive explanation](#bullet-14)<br/>\n",
    "    3.2 [XGBoost](#bullet-15)<br/>\n",
    "    3.3 [K-fold cross validation](#bullet-16)<br/>\n",
    "\n",
    "### 3.1 An intuitive explanation of gradient boosting<a class=\"anchor\" id=\"bullet-14\"></a>\n",
    "\n",
    "1. A tree is used to learn from the data as described in [section 2.2](#bullet-10). This first model is called the **base learner**.\n",
    "2. The algorithm calculates the error of the base learner's final predictions. These error measurements are called **pseudo-residuals**. \n",
    "3. A second decision tree, called the **booster**, is stacked on top of the base tree by using the pseudo-residuals as input. Its output is a prediction of error for each prediction of the base tree. \n",
    "4. The algorithm adjusts the base prediction by adding the predicted amount of error.\n",
    "*Note: target - prediction = error, so by adding 'predicted error' to the base prediction, you are getting closer to the target.*\n",
    "5. Another set of pseudo-residuals are calculated for the predictions of the updated model (base tree + first boosting tree). Each round of boosting corrects the base prediction by a predicted amount of error, gradually inching towards the true value.\n",
    "\n",
    "\n",
    "### 3.2 XGBoost<a class=\"anchor\" id=\"bullet-15\"></a>\n",
    "\n",
    "You use the XGBoost library, an implementation of gradient tree boosting popularized by usage in machine learning competitions. It is a valuable addition to any machine learning toolkit, as it significantly outperforms other algorithms in speed, accuracy, and flexibility. \n",
    "\n",
    "If you don't have xgboost library installed, install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/conda3/lib/python3.5/site-packages (0.72.1)\n",
      "Requirement already satisfied: scipy in /opt/conda3/lib/python3.5/site-packages (from xgboost) (1.1.0)\n",
      "Requirement already satisfied: numpy in /opt/conda3/lib/python3.5/site-packages (from xgboost) (1.14.5)\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note:** The xgboost package uses an older version of sklearn. When you run import xgboost, ignore the DeprecationWarning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.3 K-fold cross validation<a class=\"anchor\" id=\"bullet-16\"></a>\n",
    "\n",
    "Cross-validation is used to measure how well a model generalizes using the training set (no need to bring in the testing set yet!). The k-fold method divides the training data into even smaller train/test sets to gauge how the model performs on 'unseen' data.\n",
    "\n",
    "1. The data is divided into k subsamples. You use 5 subsamples: you can call them A, B, C, D, E.\n",
    "2. The model is trained on 4 subsamples (for example, A, B, C, D).\n",
    "3. The model makes predictions on the subsample that was held out from training (E), and you score it based on an evaluation metric.\n",
    "4. The model is then trained on another combination of subsamples (for example, B, C, D, E), and tested on the one that was held out (A).\n",
    "5. This repeats for every possible combination of k-folds. You take the average of the evaluation metrics at each iteration to give an idea of how the model performs on actual unseen data.\n",
    "\n",
    "You use cross-validation during parameter tuning so that you can directly observe the effect of a parameter on a model's generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4.0 XGBoost: Parameter tuning<a class=\"anchor\" id=\"bullet-17\"></a><a href=\"#footnote-1\"><sup>[1]</sup></a>\n",
    "\n",
    "4.1 [Default model](#bullet-18)<br/>\n",
    "    4.2 [Tuning number of estimators](#bullet-19)<br/>\n",
    "    4.3 [Evaluating default model](#bullet-20)<br/>\n",
    "    4.4 [Interpreting evaluation](#bullet-21)<br/>\n",
    "    4.5 [Grid search](#bullet-22)<br/>\n",
    "    4.6 [Tuning tree depth and min child weight](#bullet-23)<br/>\n",
    "    4.7 [Tuning gamma](#bullet-24)<br/>\n",
    "    4.8 [Evaluating updated model](#bullet-25)<br/>\n",
    "    4.9 [Tuning sampling parameters](#bullet-26)<br/>\n",
    "    4.10 [Tuning lambda & alpha](#bullet-27)<br/>\n",
    "    4.11 [Evaluating final model](#bullet-28)<br/>\n",
    "    4.12 [Predict on the test set](#bullet-29)<br/>\n",
    "\n",
    "### 4.1 Setting up a default model<a class=\"anchor\" id=\"bullet-18\"></a>\n",
    "You need to initialize a model with some default parameters as a starting point. The selection here depends on the nature of your data and your experience. Recall that parameters tell the algorithm how to learn in one of two ways: how complex the model will be, and how quickly it should conclude on a pattern in the data.\n",
    "\n",
    "Luckily, XGBoost performs well with its default values, so you only need to define the following three parameters: \n",
    "\n",
    "1. objective: Depends on the target type - use binary:logistic because you have a binary classification problem.\n",
    "2. learning rate: This parameter is always chosen first, as the optimal values of all other parameters are dependent upon it. It directly controls how conservative the model is in picking up patterns - a smaller value makes the model more conservative. A good rule of thumb is to initialize the learning rate at 0.1.  \n",
    "3. number of estimators: The maximum number of boosting trees to be added. Pick a number that is a bit larger than necessary for our data set size. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# initializing our first model with an objective and learning rate\n",
    "xgb0 = XGBClassifier(\n",
    " objective= 'binary:logistic',\n",
    " learning_rate = 0.1, \n",
    " n_estimators = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.2 Tuning number of estimators<a class=\"anchor\" id=\"bullet-19\"></a>\n",
    "\n",
    "The 'number of estimators' parameter in XGBoost refers to the maximum number of boosting trees (or rounds) to be used in building the model. Intuitively, the more trees that are added to the model, the more complex it is. This parameter is tuned first, so you can broadly adjust the complexity of your model before making adjustments that have smaller impacts on conservative learning.\n",
    "\n",
    "The function below performs the following actions to find the best number of boosting trees to use on your data:\n",
    "\n",
    "1. Trains an XGBoost model using features of the data.\n",
    "2. Performs k-fold cross validation on the model, using accuracy and AUC score as the evaluation metric.\n",
    "3. Returns output for each boosting round so you can see how the model is learning. You will look at the detailed output in the next section.\n",
    "4. It stops running after the cross-validation score does not improve significantly with additional boosting rounds, giving you an optimal number of estimators for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def evaluate_model(alg, train, target, predictors, cv_folds=5, early_stopping_rounds=1):\n",
    "    \n",
    "    xgb_param = alg.get_xgb_params()\n",
    "    xgtrain = xgb.DMatrix(train[predictors].values, target['diagnosis'].values)\n",
    "    cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "        metrics='auc', early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n",
    "    alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(train[predictors], target['diagnosis'], eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(train[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(train[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(target['diagnosis'].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(target['diagnosis'], dtrain_predprob))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importance', color='g')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.3 Evaluating the default model<a class=\"anchor\" id=\"bullet-20\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# a list of features to be used for training the model\n",
    "features = [x for x in X_train.columns if x not in ['id']]\n",
    "\n",
    "# evaluating the first model\n",
    "evaluate_model(xgb0, X_train, y_train, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.4 Interpreting evaluation output<a class=\"anchor\" id=\"bullet-21\"></a>\n",
    "\n",
    "* The first set of readings shows you the cross-validation scores at each boosting round. So at line [0], one tree has been fit to the data. Train-auc gives you the average AUC score for the five training sets during cross-validation, and test-auc gives you the same metric for the five testing sets during cross validation. Naturally, the test score is lower, as the model is predicting on data that was not used to train it. You can see that the test-auc catches up with more boosting rounds, as more trees are used to minimize the error.\n",
    "<br/><br/>\n",
    "* The model stops training after ten rounds, which means that the cross-validation scores are not significantly improving with more than 10 rounds. For a learning rate of 0.1, you can take the number of estimators to be 10 to save computation time.\n",
    "<br/><br/>\n",
    "* Overall, the accuracy is 0.9824, which tells you how many cases were correctly classified, both benign and malignant.\n",
    "<br/><br/>\n",
    "* Overall, the AUC score is 0.99877, which indicates that the model rarely lets malignant cases slip through its screening process.\n",
    "<br/><br/>\n",
    "* The graph displays the features ranked by their importance to the model. Feature importance refers to the number of times it is used as a splitting feature in the decision trees. This function is especially useful in large datasets, where many variables are just noise and <a href=\"https://en.wikipedia.org/wiki/Feature_engineering\" target=\"_blank\" rel=\"noopener no referrer\">feature engineering</a> may be required.\n",
    "\n",
    "### 4.5 Grid search<a class=\"anchor\" id=\"bullet-22\"></a>\n",
    "To tune the remaining XGBoost parameters, use grid search with cross-validation.\n",
    "\n",
    "1. Define set of values for the parameter in question. These values of interest again depend on the nature of the data and your experience.\n",
    "2. The grid search function systematically produces a model for each unique combination of these values, and evaluates it using cross-validation.\n",
    "3. The optimal parameter values are found in the model with the highest cv score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.6 Tuning max depth and child weight<a class=\"anchor\" id=\"bullet-23\"></a>\n",
    "\n",
    "**max depth**: The size of each new decision tree. Smaller trees = less complexity. <br/>\n",
    "**min child weight**: The minimum number of observations that must be in the children after a split. Smaller weight = more conservative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'max_depth': 3, 'min_child_weight': 1},\n",
       "  {'max_depth': 3, 'min_child_weight': 2},\n",
       "  {'max_depth': 4, 'min_child_weight': 1},\n",
       "  {'max_depth': 4, 'min_child_weight': 2},\n",
       "  {'max_depth': 5, 'min_child_weight': 1},\n",
       "  {'max_depth': 5, 'min_child_weight': 2},\n",
       "  {'max_depth': 6, 'min_child_weight': 1},\n",
       "  {'max_depth': 6, 'min_child_weight': 2}],\n",
       " {'max_depth': 5, 'min_child_weight': 2},\n",
       " 0.98292053663570678)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# updating our default model with the optimal number of estimators\n",
    "xgb1 = XGBClassifier(\n",
    " objective = 'binary:logistic',\n",
    " learning_rate =0.1,\n",
    " n_estimators=10)\n",
    "\n",
    "# array of values for max_depth and min_child_weight parameters\n",
    "param_test1 = {'max_depth':range(3,7,1),'min_child_weight':range(1,3,1)}\n",
    "\n",
    "# grid search with cross-validation using the updated model and parameter value array \n",
    "gsearch1 = GridSearchCV(estimator = xgb1, param_grid = param_test1, scoring='roc_auc',iid=False, cv=5)\n",
    "gsearch1.fit(X_train[features],y_train['diagnosis'])\n",
    "gsearch1.cv_results_['params'], gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The grid search found that the model works best with a max depth of 5, and a minimum child weight of 2. You can update our model accordingly and continue tuning.\n",
    "\n",
    "### 4.7 Tuning gamma<a class=\"anchor\" id=\"bullet-24\"></a>\n",
    "Increasing gamma makes the algorithm more conservative (less prone to overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'gamma': 0.0},\n",
       "  {'gamma': 0.01},\n",
       "  {'gamma': 0.02},\n",
       "  {'gamma': 0.03},\n",
       "  {'gamma': 0.04},\n",
       "  {'gamma': 0.05}],\n",
       " {'gamma': 0.03},\n",
       " 0.98323013415892646)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# updating our current model with the max_depth and min_child weight parameter values found in last grid search\n",
    "xgb2 = XGBClassifier(\n",
    " objective='binary:logistic',\n",
    " learning_rate =0.1,\n",
    " n_estimators=10,\n",
    " max_depth=5,\n",
    " min_child_weight=2,\n",
    " gamma=0)\n",
    "\n",
    "# array of values for the gamma parameter\n",
    "gamma_test = {'gamma':[i/100.0 for i in range(0,6)]}\n",
    "\n",
    "# grid search with cross-validation using the updated model and gamma value array \n",
    "gsearch2 = GridSearchCV(estimator = xgb2, param_grid = gamma_test, scoring='roc_auc',iid=False, cv=5)\n",
    "gsearch2.fit(X_train[features],y_train['diagnosis'])\n",
    "gsearch2.cv_results_['params'], gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The grid search found that the model works best when gamma is 0.03."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.8 Evaluating updated model<a class=\"anchor\" id=\"bullet-25\"></a>\n",
    "\n",
    "Revisit your evaluation function to see if you need to update the number of boosting rounds. Use the parameter values found for max_depth, min_child_weight, and gamma, and set the number of estimators as 30 to see when it stops running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.991262+0.00415659\ttest-auc:0.946294+0.0345051\n",
      "[1]\ttrain-auc:0.994621+0.00305501\ttest-auc:0.957756+0.0276168\n",
      "[2]\ttrain-auc:0.995222+0.00271398\ttest-auc:0.959676+0.0263217\n",
      "[3]\ttrain-auc:0.996267+0.00253939\ttest-auc:0.964516+0.0208037\n",
      "[4]\ttrain-auc:0.997576+0.00194061\ttest-auc:0.971918+0.0146403\n",
      "[5]\ttrain-auc:0.998328+0.000817829\ttest-auc:0.973667+0.0151064\n",
      "[6]\ttrain-auc:0.998653+0.000721709\ttest-auc:0.977446+0.0136371\n",
      "[7]\ttrain-auc:0.998747+0.000665641\ttest-auc:0.978145+0.0139851\n",
      "[8]\ttrain-auc:0.998826+0.00072572\ttest-auc:0.979183+0.0134154\n",
      "[9]\ttrain-auc:0.999054+0.000430744\ttest-auc:0.98039+0.013054\n",
      "[10]\ttrain-auc:0.999119+0.000471885\ttest-auc:0.980871+0.0135194\n",
      "[11]\ttrain-auc:0.999183+0.000410013\ttest-auc:0.981016+0.0137015\n",
      "[12]\ttrain-auc:0.999287+0.000328012\ttest-auc:0.981277+0.0140524\n",
      "[13]\ttrain-auc:0.999364+0.000266412\ttest-auc:0.982261+0.0137598\n",
      "[14]\ttrain-auc:0.999358+0.000267429\ttest-auc:0.982305+0.0138369\n",
      "[15]\ttrain-auc:0.999383+0.000249224\ttest-auc:0.982511+0.0137759\n",
      "[16]\ttrain-auc:0.999486+0.000257412\ttest-auc:0.982783+0.0138132\n",
      "[17]\ttrain-auc:0.999499+0.000251081\ttest-auc:0.982979+0.0138683\n",
      "[18]\ttrain-auc:0.999531+0.000219561\ttest-auc:0.982986+0.0133084\n",
      "[19]\ttrain-auc:0.999561+0.000221285\ttest-auc:0.985749+0.00940029\n",
      "[20]\ttrain-auc:0.99965+0.00020898\ttest-auc:0.986083+0.00980749\n",
      "[21]\ttrain-auc:0.999702+0.000174749\ttest-auc:0.986739+0.00927768\n",
      "[22]\ttrain-auc:0.999715+0.000164634\ttest-auc:0.987268+0.0088093\n",
      "\n",
      "Model Report\n",
      "Accuracy : 0.9912\n",
      "AUC Score (Train): 0.999649\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAF5CAYAAACMZh3gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ni6kn/6KGCwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b513692d128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb_check = XGBClassifier(\n",
    " objective='binary:logistic',\n",
    " learning_rate =0.1,\n",
    " n_estimators=30,\n",
    " max_depth=5,\n",
    " min_child_weight=2,\n",
    " gamma=0.03)\n",
    "\n",
    "evaluate_model(xgb_check, X_train, y_train, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.9 Tuning sampling parameters<a class=\"anchor\" id=\"bullet-26\"></a>\n",
    "\n",
    "**subsample**: The ratio of data that is randomly selected for growing trees<br/>\n",
    "**colsample_bytree**: The ratio of the subsample from which the algorithm selects splitting features\n",
    "\n",
    "**Note:** You have reduced your data set several times now; originally, you divided it into 80/20 train and test sets. You input only the training set to XGBoost, which are segmented twice more by subsample and colsample. Perhaps less than 50% of the original data will be used to train the model, reducing the chance of overfitting and improving the model's ability to generalize. It is one of the reasons why you see a better test score for XGBoost compared to the single decision tree model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'colsample_bytree': 0.5, 'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.5, 'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.5, 'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.5, 'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.6, 'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.6, 'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.6, 'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.6, 'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.7, 'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.7, 'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.7, 'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.7, 'subsample': 0.8},\n",
       "  {'colsample_bytree': 0.8, 'subsample': 0.5},\n",
       "  {'colsample_bytree': 0.8, 'subsample': 0.6},\n",
       "  {'colsample_bytree': 0.8, 'subsample': 0.7},\n",
       "  {'colsample_bytree': 0.8, 'subsample': 0.8}],\n",
       " {'colsample_bytree': 0.5, 'subsample': 0.7},\n",
       " 0.99164086687306496)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# updating our current model with the most recent n_estimators\n",
    "xgb3 = XGBClassifier(\n",
    " objective='binary:logistic',\n",
    " learning_rate =0.1,\n",
    " n_estimators=23,\n",
    " max_depth=5,\n",
    " min_child_weight=2,\n",
    " gamma=0.03)\n",
    "\n",
    "# array of values for subsample and colsample_bytree parameters\n",
    "sample_test = {\n",
    " 'subsample':[i/10.0 for i in range(5,9)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(5,9)]\n",
    "}\n",
    "\n",
    "# grid search with cross validation for sampling parameters\n",
    "gsearch3 = GridSearchCV(estimator = xgb3, param_grid = sample_test, scoring='roc_auc',iid=False, cv=5)\n",
    "gsearch3.fit(X_train[features],y_train['diagnosis'])\n",
    "gsearch3.cv_results_['params'], gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The grid search found that the model works best with a colsample_bytree of 0.5, and a subsample of 0.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.10 Tuning lambda and alpha<a class=\"anchor\" id=\"bullet-27\"></a>\n",
    "\n",
    "Lambda and alpha are both regularization parameters, which mathematically reduce the impact of features that might be too dominant in the model. The difference between the two is in how they apply these penalties.\n",
    "\n",
    "**lambda**: Applies L2 regularization, which shrinks the weights of all selected features equally. The default value is 0. <br/>\n",
    "**alpha**: Applies L1 regularization, which can shrink the weights down to 0 - essentially discarding features that have little impact on the model. The default is 1.\n",
    "\n",
    "These parameters have more significant effects on large models (with more features). Tune them to see how it impacts your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'reg_alpha': 0, 'reg_lambda': 1},\n",
       "  {'reg_alpha': 0, 'reg_lambda': 1.1},\n",
       "  {'reg_alpha': 0, 'reg_lambda': 1.2},\n",
       "  {'reg_alpha': 0, 'reg_lambda': 1.3},\n",
       "  {'reg_alpha': 0.01, 'reg_lambda': 1},\n",
       "  {'reg_alpha': 0.01, 'reg_lambda': 1.1},\n",
       "  {'reg_alpha': 0.01, 'reg_lambda': 1.2},\n",
       "  {'reg_alpha': 0.01, 'reg_lambda': 1.3},\n",
       "  {'reg_alpha': 0.1, 'reg_lambda': 1},\n",
       "  {'reg_alpha': 0.1, 'reg_lambda': 1.1},\n",
       "  {'reg_alpha': 0.1, 'reg_lambda': 1.2},\n",
       "  {'reg_alpha': 0.1, 'reg_lambda': 1.3}],\n",
       " {'reg_alpha': 0.01, 'reg_lambda': 1},\n",
       " 0.99184726522187816)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# updating our current model with the sampling parameters found in the last grid search\n",
    "xgb4 = XGBClassifier(\n",
    " objective='binary:logistic',\n",
    " learning_rate =0.1,\n",
    " n_estimators=23,\n",
    " max_depth=5,\n",
    " min_child_weight=2,\n",
    " gamma=0.03, \n",
    " subsample=0.7,\n",
    " colsample_bytree=0.5)\n",
    "\n",
    "# array of values for subsample and colsample_bytree parameters\n",
    "reg_test = {'reg_alpha':[0, 0.01, 0.1], 'reg_lambda':[1, 1.1, 1.2, 1.3]}\n",
    "\n",
    "# grid search with cross validation for regularization parameters\n",
    "gsearch4 = GridSearchCV(estimator = xgb4, param_grid = reg_test, scoring='roc_auc',iid=False, cv=5)\n",
    "gsearch4.fit(X_train[features],y_train['diagnosis'])\n",
    "gsearch4.cv_results_['params'], gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The grid search found that the model works best when the reg_alpha regualization is 0.01 and the reg_lambda regulization is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.11 Evaluating final model<a class=\"anchor\" id=\"bullet-28\"></a>\n",
    "Run the current model through your evaluation function a final time. Compared to the model with default parameters, tuning has improved accuracy from 0.9824 to 0.9912, and the AUC score from 0.998772 to 0.999195. You can see why XGBoost is most popular for competition use, where those fourth decimal places really count!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.991262+0.00415659\ttest-auc:0.946294+0.0345051\n",
      "[1]\ttrain-auc:0.994621+0.00305501\ttest-auc:0.957756+0.0276168\n",
      "[2]\ttrain-auc:0.995222+0.00271398\ttest-auc:0.959676+0.0263217\n",
      "[3]\ttrain-auc:0.996267+0.00253939\ttest-auc:0.964516+0.0208037\n",
      "[4]\ttrain-auc:0.997576+0.00194061\ttest-auc:0.971918+0.0146403\n",
      "[5]\ttrain-auc:0.997686+0.00194212\ttest-auc:0.973667+0.0151064\n",
      "[6]\ttrain-auc:0.99863+0.000761627\ttest-auc:0.977549+0.0137967\n",
      "[7]\ttrain-auc:0.998715+0.000685681\ttest-auc:0.977856+0.0140668\n",
      "[8]\ttrain-auc:0.998887+0.000614441\ttest-auc:0.97879+0.0133742\n",
      "[9]\ttrain-auc:0.999005+0.000496548\ttest-auc:0.980274+0.0131517\n",
      "[10]\ttrain-auc:0.999145+0.000393217\ttest-auc:0.980969+0.0135316\n",
      "[11]\ttrain-auc:0.999228+0.000359222\ttest-auc:0.981501+0.0138705\n",
      "[12]\ttrain-auc:0.999298+0.000292906\ttest-auc:0.981847+0.0138087\n",
      "[13]\ttrain-auc:0.999441+0.000257661\ttest-auc:0.982336+0.013756\n",
      "\n",
      "Model Report\n",
      "Accuracy : 0.9912\n",
      "AUC Score (Train): 0.999195\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAF5CAYAAACSry1TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4dbD/gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b51369176d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# updated model with alpha value found with last grid search. (lambda does not need to be set, since default is 1)\n",
    "xgb_final = XGBClassifier(\n",
    " objective='binary:logistic',\n",
    " learning_rate =0.1,\n",
    " n_estimators=30,\n",
    " max_depth=5,\n",
    " min_child_weight=2,\n",
    " gamma=0.03,\n",
    " reg_alpha=0.01)\n",
    "\n",
    "evaluate_model(xgb_final, X_train, y_train, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.12 Predict on the test set<a class=\"anchor\" id=\"bullet-29\"></a>\n",
    "Finally, predict on the test data you reserved at the beginning. The XGBoost ensemble gives you approximately a 3% increase in accuracy and a 5% increase in AUC score over a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9561\n",
      "AUC Score (Test): 0.981647\n"
     ]
    }
   ],
   "source": [
    "test_features = [x for x in X_test.columns if x not in ['id']]\n",
    "\n",
    "#Predict test set:\n",
    "test_predictions = xgb_final.predict(X_test[test_features])\n",
    "test_predprob = xgb_final.predict_proba(X_test[test_features])[:,1]\n",
    "        \n",
    "#Print model report:\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_test['diagnosis'].values, test_predictions))\n",
    "print(\"AUC Score (Test): %f\" % metrics.roc_auc_score(y_test['diagnosis'], test_predprob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you want to increase the accuracy and AUC score even further, you can restart the parameter tuning process with a smaller learning rate. The next value to try is 0.01. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Summary\n",
    "You've found out how to use a decision tree as a machine learning model to classify cancer data, and you've seen how you how tools like XGBoost can be used to tune parameters to improve the accuracy of your model.\n",
    "\n",
    "### Learn more:\n",
    "* <p id=\"footnote-1\"><sup>[1]</sup><a id=\"fn1\" href=\"https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python\" target=\"_blank\" rel=\"noopener noreferrer\">Jain, A. (March 1, 2016) Complete Guide to Parameter Tuning in XGBoost(with codes in Python)</a>\n",
    "* <a href=\"https://en.wikipedia.org/wiki/Feature_engineering\" target=\"_blank\" rel=\"noopener no referrer\">Feature engineering</a>\n",
    "* <a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning#Metrics\" target=\"_blank\" rel=\"noopener noreferrer\">Decision tree learning metrics</a>\n",
    "* <a href=\"https://www.quora.com/Machine-Learning-What-is-an-intuitive-explanation-of-AUC\" rel=\"noopener noreferrer\">An intuitive explanation of AUC</a>\n",
    "* <a href=\"http://www.wikihow.com/Play-20-Questions\" target=\"_blank\" rel=\"noopener noreferrer\">How to play 20 questions</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Author\n",
    "Natalie Ho studies statistics at the University of Waterloo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data citations\n",
    "\n",
    "<p id=\"footnote-2\"> Lichman, M. (2013). [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science. <a id=\"fn2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Copyright © IBM Corp. 2017, 2018. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
