{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################################################################\n",
    "#Licensed Materials - Property of IBM\n",
    "#(C) Copyright IBM Corp. 2020\n",
    "#US Government Users Restricted Rights - Use, duplication disclosure restricted\n",
    "#by GSA ADP Schedule Contract with IBM Corp.\n",
    "################################################################################\n",
    "\n",
    "The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for Watson Studio Auto-generated Notebook (License Terms), such agreements located in the link below.\n",
    "Specifically, the Source Components and Sample Materials clause included in the License Information document for\n",
    "Watson Studio Auto-generated Notebook applies to the auto-generated notebooks. \n",
    "By downloading, copying, accessing, or otherwise using the materials, you agree to the License Terms.\n",
    "http://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BHU2B7&title=IBM%20Watson%20Studio%20Auto-generated%20Notebook%20V2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## IBM AutoAI Auto-Generated Notebook v1.11.10\n",
    "**Note**: Notebook code generated using AutoAI will execute successfully.\n",
    "If code is modified or reordered, there is no guarantee it will successfully execute.\n",
    "This pipeline is optimized for the original dataset.The pipeline may fail or produce sub-optimium results if used with different data.\n",
    "For different data, please consider returning to AutoAI Experiments to generate a new pipeline.\n",
    "Please read our documentation for more information: \n",
    "(Cloud Platform) https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/autoai-notebook.html .\n",
    "(Cloud Pak For Data) https://www.ibm.com/support/knowledgecenter/SSQNUZ_3.0.0/wsj/analyze-data/autoai-notebook.html .\n",
    "\n",
    "Before modifying the pipeline or trying to re-fit the pipeline, consider: \n",
    "The notebook converts dataframes to numpy arrays before fitting the pipeline (a current restriction of the preprocessor pipeline).\n",
    "The known_values_list is passed by reference and populated with categorical values during fit of the preprocessing pipeline.Delete its members before re-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing Pipeline from run: Pipeline_4 from run baa0748bbbbf4149acfbb5e95543fbb7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    import autoai_libs\n",
    "except Exception as e:\n",
    "    import subprocess\n",
    "    out = subprocess.check_output('pip install autoai-libs'.split(' '))\n",
    "    for line in out.splitlines():\n",
    "        print(line)\n",
    "    import autoai_libs\n",
    "import sklearn\n",
    "try:\n",
    "    import xgboost\n",
    "except:\n",
    "    print('xgboost, if needed, will be installed and imported later')\n",
    "try:\n",
    "    import lightgbm\n",
    "except:\n",
    "    print('lightgbm, if needed, will be installed and imported later')\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "import numpy\n",
    "from numpy import inf, nan, dtype, mean\n",
    "from autoai_libs.sklearn.custom_scorers import CustomScorers\n",
    "import sklearn.ensemble\n",
    "from autoai_libs.cognito.transforms.transform_utils import TExtras, FC\n",
    "from autoai_libs.transformers.exportable import *\n",
    "from autoai_libs.utils.exportable_utils import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "known_values_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose a decorator to assist pipeline instantiation via import of modules and installation of packages\n",
    "def decorator_retries(func):\n",
    "    def install_import_retry(*args, **kwargs):\n",
    "        retries = 0\n",
    "        successful = False\n",
    "        failed_retries = 0\n",
    "        while retries < 100 and failed_retries < 10 and not successful:\n",
    "            retries += 1\n",
    "            failed_retries += 1\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                successful = True\n",
    "            except Exception as e:\n",
    "                estr = str(e)\n",
    "                if estr.startswith('name ') and estr.endswith(' is not defined'):\n",
    "                    try:\n",
    "                        import importlib\n",
    "                        module_name = estr.split(\"'\")[1]\n",
    "                        module = importlib.import_module(module_name)\n",
    "                        globals().update({module_name: module})\n",
    "                        print('import successful for ' + module_name)\n",
    "                        failed_retries -= 1\n",
    "                    except Exception as import_failure:\n",
    "                        print('import of ' + module_name + ' failed with: ' + str(import_failure))\n",
    "                        import subprocess\n",
    "                        print('attempting pip install of ' + module_name)\n",
    "                        process = subprocess.Popen('pip install ' + module_name, shell=True)\n",
    "                        process.wait()\n",
    "                        try:\n",
    "                            print('re-attempting import of ' + module_name)\n",
    "                            module = importlib.import_module(module_name)\n",
    "                            globals().update({module_name: module})\n",
    "                            print('import successful for ' + module_name)\n",
    "                            failed_retries -= 1\n",
    "                        except Exception as import_or_installation_failure:\n",
    "                            print('failure installing and/or importing ' + module_name + ' error was: ' + str(\n",
    "                                import_or_installation_failure))\n",
    "                            raise (ModuleNotFoundError('Missing package in environment for ' + module_name +\n",
    "                                                       '? Try import and/or pip install manually?'))\n",
    "                elif type(e) is AttributeError:\n",
    "                    if 'module ' in estr and ' has no attribute ' in estr:\n",
    "                        pieces = estr.split(\"'\")\n",
    "                        if len(pieces) == 5:\n",
    "                            try:\n",
    "                                import importlib\n",
    "                                print('re-attempting import of ' + pieces[3] + ' from ' + pieces[1])\n",
    "                                module = importlib.import_module('.' + pieces[3], pieces[1])\n",
    "                                failed_retries -= 1\n",
    "                            except:\n",
    "                                print('failed attempt to import ' + pieces[3])\n",
    "                                raise (e)\n",
    "                        else:\n",
    "                            raise (e)\n",
    "                else:\n",
    "                    raise (e)\n",
    "        if successful:\n",
    "            print('Pipeline successfully instantiated')\n",
    "        else:\n",
    "            raise (ModuleNotFoundError(\n",
    "                'Remaining missing imports/packages in environment? Retry cell and/or try pip install manually?'))\n",
    "        return result\n",
    "    return install_import_retry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compose Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata necessary to replicate AutoAI scores with the pipeline\n",
    "_input_metadata = {'target_label_name': 'Risk', 'learning_type': 'classification', 'run_uid': 'baa0748bbbbf4149acfbb5e95543fbb7', 'pn': 'P4', 'cv_num_folds': 3, 'holdout_fraction': 0.1, 'optimization_metric': 'roc_auc', 'pos_label': None, 'random_state': 33, 'data_source': ''}\n",
    "\n",
    "# define a function to compose the pipeline, and invoke it\n",
    "@decorator_retries\n",
    "def compose_pipeline():\n",
    "    import numpy\n",
    "    from numpy import nan, dtype, mean\n",
    "    #\n",
    "    # composing steps for toplevel Pipeline\n",
    "    #\n",
    "    _input_metadata = {'target_label_name': 'Risk', 'learning_type': 'classification', 'run_uid': 'baa0748bbbbf4149acfbb5e95543fbb7', 'pn': 'P4', 'cv_num_folds': 3, 'holdout_fraction': 0.1, 'optimization_metric': 'roc_auc', 'pos_label': None, 'random_state': 33, 'data_source': ''}\n",
    "    steps = []\n",
    "    #\n",
    "    # composing steps for preprocessor Pipeline\n",
    "    #\n",
    "    preprocessor__input_metadata = None\n",
    "    preprocessor_steps = []\n",
    "    #\n",
    "    # composing steps for preprocessor_features FeatureUnion\n",
    "    #\n",
    "    preprocessor_features_transformer_list = []\n",
    "    #\n",
    "    # composing steps for preprocessor_features_categorical Pipeline\n",
    "    #\n",
    "    preprocessor_features_categorical__input_metadata = None\n",
    "    preprocessor_features_categorical_steps = []\n",
    "    preprocessor_features_categorical_steps.append(('cat_column_selector', autoai_libs.transformers.exportable.NumpyColumnSelector(columns=[0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])))\n",
    "    preprocessor_features_categorical_steps.append(('cat_compress_strings', autoai_libs.transformers.exportable.CompressStrings(activate_flag=True, compress_type='hash', dtypes_list=['char_str', 'int_num', 'char_str', 'char_str', 'char_str', 'char_str', 'int_num', 'char_str', 'char_str', 'int_num', 'char_str', 'int_num', 'char_str', 'char_str', 'int_num', 'char_str', 'int_num', 'char_str', 'char_str'], missing_values_reference_list=['', '-', '?', nan], misslist_list=[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []])))\n",
    "    preprocessor_features_categorical_steps.append(('cat_missing_replacer', autoai_libs.transformers.exportable.NumpyReplaceMissingValues(filling_values=nan, missing_values=[])))\n",
    "    preprocessor_features_categorical_steps.append(('cat_unknown_replacer', autoai_libs.transformers.exportable.NumpyReplaceUnknownValues(filling_values=nan, filling_values_list=[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], known_values_list=[[227259264688753646810077375790908286508, 253732214910815238134509288111402486722, 303819144345098626554456011496217223575, 280353606872939388614315901186094326949], [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 59, 60, 61, 62, 63, 64], [310861434292724266828512742170106879235, 259078546728323006823621149382123274851, 241304191515355600141369672905481462228, 210421363963937264134596964576525922958, 32212939341074532066845731816970003693], [338473629843581165720544432281125239609, 326781793206258442673692969623843435113, 272002530391893084684429324616350007975, 249661578098852569030863268336475104259, 277452992311061223498548004061740954923, 337000624133206789825115182809991844205, 161330870025401753341235738137984802479, 133401727367742042681318083577551821203, 260491112873202394519945523636494874646, 208131593747174447223154626949015695834, 317093190772141179412539616679296858597], [155466114539413991851582417126364895339, 246789352403109329930648176329125946981, 295936621451169689699218469248254275361, 223681476361652455808150317408145666122, 230715114430321724850934713783197932106], [185340558928138909626603812763488190588, 319328661378046583715176476627492385778, 263799749086692576083148855705354099872, 179222502770383138876302755836424893586, 283364312271660996400883763491949419861], [1, 2, 3, 4, 5, 6], [52149379001264932068757487059177351405, 10381015089147753033583386570985939629], [90380513159839424205657141466603309780, 72020144360318906788270974425375628494, 68186749286663113704472210246844540664], [1, 2, 3, 4, 5, 6], [184210183797580735197442541762508088723, 112784645159752486323086095897586579102, 194064631024616485005022213750224679171, 230715114430321724850934713783197932106], [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 73, 74], [251696305476606261219998849299484831626, 68186749286663113704472210246844540664, 129843969358953857049997737920307864573], [226204649993248704235747196731638161812, 240703879997496844699455009880498205665, 243184888600665221836895697954701357177], [1, 2, 3, 4], [328286527295663582663365503319902632676, 119641707607939038914465000864290288880, 283364312271660996400883763491949419861, 27741019508977055807423991753468819528], [1, 2], [68186749286663113704472210246844540664, 220736790854050750400968561922076059550], [169662019754859674907370307324476606919, 220736790854050750400968561922076059550]], missing_values_reference_list=['', '-', '?', nan])))\n",
    "    preprocessor_features_categorical_steps.append(('boolean2float_transformer', autoai_libs.transformers.exportable.boolean2float(activate_flag=True)))\n",
    "    preprocessor_features_categorical_steps.append(('cat_imputer', autoai_libs.transformers.exportable.CatImputer(activate_flag=True, missing_values=nan, sklearn_version_family='20', strategy='most_frequent')))\n",
    "    preprocessor_features_categorical_steps.append(('cat_encoder', autoai_libs.transformers.exportable.CatEncoder(activate_flag=True, categories='auto', dtype=numpy.float64, encoding='ordinal', handle_unknown='error', sklearn_version_family='20')))\n",
    "    preprocessor_features_categorical_steps.append(('float32_transformer', autoai_libs.transformers.exportable.float32_transform(activate_flag=True)))\n",
    "    # assembling preprocessor_features_categorical_ Pipeline\n",
    "    preprocessor_features_categorical_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_features_categorical_steps)\n",
    "    preprocessor_features_transformer_list.append(('categorical', preprocessor_features_categorical_pipeline))\n",
    "    #\n",
    "    # composing steps for preprocessor_features_numeric Pipeline\n",
    "    #\n",
    "    preprocessor_features_numeric__input_metadata = None\n",
    "    preprocessor_features_numeric_steps = []\n",
    "    preprocessor_features_numeric_steps.append(('num_column_selector', autoai_libs.transformers.exportable.NumpyColumnSelector(columns=[4])))\n",
    "    preprocessor_features_numeric_steps.append(('num_floatstr2float_transformer', autoai_libs.transformers.exportable.FloatStr2Float(activate_flag=True, dtypes_list=['int_num'], missing_values_reference_list=[])))\n",
    "    preprocessor_features_numeric_steps.append(('num_missing_replacer', autoai_libs.transformers.exportable.NumpyReplaceMissingValues(filling_values=nan, missing_values=[])))\n",
    "    preprocessor_features_numeric_steps.append(('num_imputer', autoai_libs.transformers.exportable.NumImputer(activate_flag=True, missing_values=nan, strategy='median')))\n",
    "    preprocessor_features_numeric_steps.append(('num_scaler', autoai_libs.transformers.exportable.OptStandardScaler(num_scaler_copy=None, num_scaler_with_mean=None, num_scaler_with_std=None, use_scaler_flag=False)))\n",
    "    preprocessor_features_numeric_steps.append(('float32_transformer', autoai_libs.transformers.exportable.float32_transform(activate_flag=True)))\n",
    "    # assembling preprocessor_features_numeric_ Pipeline\n",
    "    preprocessor_features_numeric_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_features_numeric_steps)\n",
    "    preprocessor_features_transformer_list.append(('numeric', preprocessor_features_numeric_pipeline))\n",
    "    # assembling preprocessor_features_ FeatureUnion\n",
    "    preprocessor_features_pipeline = sklearn.pipeline.FeatureUnion(transformer_list=preprocessor_features_transformer_list)\n",
    "    preprocessor_steps.append(('features', preprocessor_features_pipeline))\n",
    "    preprocessor_steps.append(('permuter', autoai_libs.transformers.exportable.NumpyPermuteArray(axis=0, permutation_indices=[0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 4])))\n",
    "    # assembling preprocessor_ Pipeline\n",
    "    preprocessor_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_steps)\n",
    "    steps.append(('preprocessor', preprocessor_pipeline))\n",
    "    #\n",
    "    # composing steps for cognito Pipeline\n",
    "    #\n",
    "    cognito__input_metadata = None\n",
    "    cognito_steps = []\n",
    "    cognito_steps.append(('No_action', autoai_libs.cognito.transforms.transform_utils.TNoOp(fun=None, name='no_action', datatypes='x', feat_constraints=[], tgraph=None)))\n",
    "    # assembling cognito_ Pipeline\n",
    "    cognito_pipeline = sklearn.pipeline.Pipeline(steps=cognito_steps)\n",
    "    steps.append(('cognito', cognito_pipeline))\n",
    "    steps.append(('estimator', sklearn.ensemble.gradient_boosting.GradientBoostingClassifier(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance', max_depth=5, max_features=0.23755184912201782, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=0.010000000313740999, min_samples_split=0.2048665521116347, min_weight_fraction_leaf=0.0, n_estimators=77, n_iter_no_change=None, presort='auto', random_state=33, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False)))\n",
    "    # assembling  Pipeline\n",
    "    pipeline = sklearn.pipeline.Pipeline(steps=steps)\n",
    "    return pipeline\n",
    "pipeline = compose_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract needed parameter values from AutoAI run metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Metadata used in retrieving data and computing metrics.  Customize as necessary for your environment.\n",
    "#data_source='replace_with_path_and_csv_filename'\n",
    "target_label_name = _input_metadata['target_label_name']\n",
    "learning_type = _input_metadata['learning_type']\n",
    "optimization_metric = _input_metadata['optimization_metric']\n",
    "random_state = _input_metadata['random_state']\n",
    "cv_num_folds = _input_metadata['cv_num_folds']\n",
    "holdout_fraction = _input_metadata['holdout_fraction']\n",
    "if 'data_provenance' in _input_metadata:\n",
    "    data_provenance = _input_metadata['data_provenance']\n",
    "else:\n",
    "    data_provenance = None\n",
    "if 'pos_label' in _input_metadata and learning_type == 'classification':\n",
    "    pos_label = _input_metadata['pos_label']\n",
    "else:\n",
    "    pos_label = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create dataframe from dataset in Cloud Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @hidden_cell\n",
    "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "credentials_0 = {\n",
    "    'ENDPOINT': 'unknown cloud service endpoint',\n",
    "    'IBM_AUTH_ENDPOINT': 'unknown auth_endpoint',\n",
    "    'APIKEY': 'unspecified api_key for bucket',\n",
    "    'BUCKET': 'unspecified bucket name',\n",
    "    'FILE': 'unspecified object name',\n",
    "    'SERVICE_NAME': 'data_asset',\n",
    "    'ASSET_ID': 'credit_risk_training.csv',\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Read the data as a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "csv_encodings=['UTF-8','Latin-1'] # supplement list of encodings as necessary for your data\n",
    "df = None\n",
    "readable = None  # if automatic detection fails, you can supply a filename here\n",
    "\n",
    "# First, obtain a readable object\n",
    "# Cloud Object Storage data access\n",
    "# Assumes COS credentials are in a dictionary named 'credentials_0'\n",
    "   \n",
    "credentials = df = globals().get('credentials_0')       \n",
    "if readable is None and credentials is not None :\n",
    "    try:\n",
    "        import types\n",
    "        import pandas as pd\n",
    "        import io\n",
    "        import os\n",
    "    except Exception as import_exception:\n",
    "        print('Error with importing packages - check if you installed them on your environment')\n",
    "    try:\n",
    "        if credentials['SERVICE_NAME'] == 's3':\n",
    "            try:\n",
    "                from botocore.client import Config\n",
    "                import ibm_boto3\n",
    "            except Exception as import_exception:\n",
    "                print('Installing required packages!')\n",
    "                !pip install ibm-cos-sdk\n",
    "                print('accessing data via Cloud Object Storage')\n",
    "            try:\n",
    "                cos_client = ibm_boto3.resource(service_name=credentials['SERVICE_NAME'],\n",
    "                                                ibm_api_key_id=credentials['APIKEY'],\n",
    "                                                ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n",
    "                                                config=Config(signature_version='oauth'),\n",
    "                                                endpoint_url=credentials['ENDPOINT'])\n",
    "            except Exception as cos_exception:\n",
    "                print('unable to create client for cloud object storage')\n",
    "            try:\n",
    "                cos_client.meta.client.download_file(Bucket=credentials['BUCKET'], Filename=credentials['FILE'], Key=credentials['FILE'])\n",
    "            except Exception as cos_access_exception:\n",
    "                print('unable to access data object in cloud object storage with credentials supplied') \n",
    "            try:\n",
    "                df = pd.read_csv(credentials['FILE'])\n",
    "                os.remove(credentials['FILE'])\n",
    "            except Exception as cos_object_read_exception:\n",
    "                print('unable to access data object from cos object with path supplied') \n",
    "        elif credentials['SERVICE_NAME'] == 'fs':\n",
    "            print('accessing data via File System')\n",
    "            try:\n",
    "                df = pd.read_csv(credentials['FILE'])\n",
    "            except Exception as FS_access_exception:\n",
    "                print('unable to access data object in File System with path supplied') \n",
    "    except Exception as data_access_exception:\n",
    "        print('unable to access data object with credentials supplied') \n",
    "\n",
    "# IBM Cloud Pak for Data data access\n",
    "project_filename = globals().get('project_filename')       \n",
    "if readable is None and 'credentials_0' in globals() and 'ASSET_ID' in credentials_0:\n",
    "    project_filename = credentials_0['ASSET_ID']\n",
    "if project_filename is not None:\n",
    "    print('attempting project_lib access to ' + str(project_filename))\n",
    "    try:\n",
    "        from project_lib import Project\n",
    "        project = Project.access()\n",
    "        storage_credentials = project.get_storage_metadata()\n",
    "        readable = project.get_file(project_filename)\n",
    "    except Exception as project_exception:\n",
    "        print('unable to access data using the project_lib interface and filename supplied')\n",
    "\n",
    "        \n",
    "# Use data_provenance as filename if other access mechanisms are unsuccessful\n",
    "if readable is None and type(data_provenance) is str:\n",
    "    print('attempting to access local file using path and name ' + data_provenance)\n",
    "    readable = data_provenance\n",
    "\n",
    "# Second, use pd.read_csv to read object, iterating over list of csv_encodings until successful\n",
    "if readable is not None:\n",
    "    for encoding in csv_encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(readable, encoding=encoding)\n",
    "            print('successfully loaded dataframe using encoding = ' + str(encoding))\n",
    "            break\n",
    "        except Exception as exception_csv:\n",
    "            print('unable to read csv using encoding ' + str(encoding))\n",
    "            print('handled error was ' + str(exception_csv))\n",
    "    if df is None:\n",
    "        print('unable to read file/object as a dataframe using supplied csv_encodings ' + str(csv_encodings))\n",
    "        print(f'Please use \\'insert to code\\' on data panel to load dataframe.')\n",
    "        raise(ValueError('unable to read file/object as a dataframe using supplied csv_encodings ' + str(csv_encodings)))\n",
    "\n",
    "if isinstance(df,pd.DataFrame):\n",
    "    print('Data loaded succesfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows whose target is not defined\n",
    "target = target_label_name # your target name here\n",
    "if learning_type == 'regression':\n",
    "    df[target] = pd.to_numeric(df[target], errors='coerce')\n",
    "df.dropna('rows', how='any', subset=[target], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract X and y\n",
    "df_X = df.drop(columns=[target])\n",
    "df_y = df[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detach preprocessing pipeline (which needs to see all training data)\n",
    "preprocessor_index = -1\n",
    "preprocessing_steps = [] \n",
    "for i, step in enumerate(pipeline.steps):\n",
    "    preprocessing_steps.append(step)\n",
    "    if step[0]=='preprocessor':\n",
    "        preprocessor_index = i\n",
    "        break\n",
    "#if len(pipeline.steps) > preprocessor_index+1 and pipeline.steps[preprocessor_index + 1][0] == 'cognito':\n",
    "    #preprocessor_index += 1\n",
    "    #preprocessing_steps.append(pipeline.steps[preprocessor_index])\n",
    "if preprocessor_index >= 0:\n",
    "    preprocessing_pipeline = Pipeline(memory=pipeline.memory, steps=preprocessing_steps)\n",
    "    pipeline = Pipeline(steps=pipeline.steps[preprocessor_index+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess X\n",
    "# preprocessor should see all data for cross_validate on the remaining steps to match autoai scores\n",
    "known_values_list.clear()  #  known_values_list is filled in by the preprocessing_pipeline if needed\n",
    "preprocessing_pipeline.fit(df_X.values, df_y.values)\n",
    "X_prep = preprocessing_pipeline.transform(df_X.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Split data into Training and Holdout sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine learning_type and perform holdout split (stratify conditionally)\n",
    "if learning_type is None:\n",
    "    # When the problem type is not available in the metadata, use the sklearn type_of_target to determine whether to stratify the holdout split\n",
    "    # Caution:  This can mis-classify regression targets that can be expressed as integers as multiclass, in which case manually override the learning_type\n",
    "    from sklearn.utils.multiclass import type_of_target\n",
    "    if type_of_target(df_y.values) in ['multiclass', 'binary']:\n",
    "        learning_type = 'classification'\n",
    "    else:\n",
    "        learning_type = 'regression'\n",
    "    print('learning_type determined by type_of_target as:',learning_type)\n",
    "else:\n",
    "    print('learning_type specified as:',learning_type)\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "if learning_type == 'classification':\n",
    "    X, X_holdout, y, y_holdout = train_test_split(X_prep, df_y.values, test_size=holdout_fraction, random_state=random_state, stratify=df_y.values)\n",
    "else:\n",
    "    X, X_holdout, y, y_holdout = train_test_split(X_prep, df_y.values, test_size=holdout_fraction, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Generate features via Feature Engineering pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detach Feature Engineering pipeline if next, fit it, and transform the training data\n",
    "fe_pipeline = None\n",
    "if pipeline.steps[0][0] == 'cognito':\n",
    "    try:\n",
    "        fe_pipeline = Pipeline(steps=[pipeline.steps[0]])\n",
    "        X = fe_pipeline.fit_transform(X, y)\n",
    "        X_holdout = fe_pipeline.transform(X_holdout)\n",
    "        pipeline.steps = pipeline.steps[1:]\n",
    "    except IndexError:\n",
    "        try:\n",
    "            print('Trying to compose pipeline with some of cognito steps')\n",
    "            fe_pipeline = Pipeline(steps = list([pipeline.steps[0][1].steps[0],pipeline.steps[0][1].steps[1]]))\n",
    "            X = fe_pipeline.fit_transform(X, y)\n",
    "            X_holdout = fe_pipeline.transform(X_holdout)\n",
    "            pipeline.steps = pipeline.steps[1:]\n",
    "        except IndexError:\n",
    "            print('Composing pipeline without cognito steps!')\n",
    "            pipeline.steps = pipeline.steps[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 8. Additional setup: Define a function that returns a scorer for the target's positive label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create a function to produce a scorer for a given positive label\n",
    "def make_pos_label_scorer(scorer, pos_label):\n",
    "    kwargs = {'pos_label':pos_label}\n",
    "    for prop in ['needs_proba', 'needs_threshold']:\n",
    "        if prop+'=True' in scorer._factory_args():\n",
    "            kwargs[prop] = True\n",
    "    if scorer._sign == -1:\n",
    "        kwargs['greater_is_better'] = False\n",
    "    from sklearn.metrics import make_scorer\n",
    "    scorer=make_scorer(scorer._score_func, **kwargs)\n",
    "    return scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Fit pipeline, predict on Holdout set, calculate score, perform cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# fit the remainder of the pipeline on the training data\n",
    "pipeline.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the holdout data\n",
    "y_pred = pipeline.predict(X_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute score for the optimization metric\n",
    "# scorer may need pos_label, but not all scorers take pos_label parameter\n",
    "from sklearn.metrics import get_scorer\n",
    "scorer = get_scorer(optimization_metric)\n",
    "score = None\n",
    "#score = scorer(pipeline, X_holdout, y_holdout)  # this would suffice for simple cases\n",
    "pos_label = None  # if you want to supply the pos_label, specify it here\n",
    "if pos_label is None and 'pos_label' in _input_metadata:\n",
    "    pos_label=_input_metadata['pos_label']\n",
    "try:\n",
    "    score = scorer(pipeline, X_holdout, y_holdout)\n",
    "except Exception as e1:\n",
    "    if pos_label is None or str(pos_label)=='':\n",
    "        print('You may have to provide a value for pos_label in order for a score to be calculated.')\n",
    "        raise(e1)\n",
    "    else:\n",
    "        exception_string=str(e1)\n",
    "        if 'pos_label' in exception_string:\n",
    "            try:\n",
    "                scorer = make_pos_label_scorer(scorer, pos_label=pos_label)\n",
    "                score = scorer(pipeline, X_holdout, y_holdout)\n",
    "                print('Retry was successful with pos_label supplied to scorer')\n",
    "            except Exception as e2:\n",
    "                print('Initial attempt to use scorer failed.  Exception was:')\n",
    "                print(e1)\n",
    "                print('')\n",
    "                print('Retry with pos_label failed.  Exception was:')\n",
    "                print(e2)\n",
    "        else:\n",
    "            raise(e1)\n",
    "\n",
    "if score is not None:\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_validate pipeline using training data\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "if learning_type == 'classification':\n",
    "    fold_generator = StratifiedKFold(n_splits=cv_num_folds, random_state=random_state)\n",
    "else:\n",
    "    fold_generator = KFold(n_splits=cv_num_folds, random_state=random_state)\n",
    "cv_results = cross_validate(pipeline, X, y, cv=fold_generator, scoring={optimization_metric:scorer}, return_train_score=True)\n",
    "import numpy as np\n",
    "np.mean(cv_results['test_' + optimization_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
